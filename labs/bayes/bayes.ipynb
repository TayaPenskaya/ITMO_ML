{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(filename):\n",
    "    legit = 0\n",
    "    if \"legit\" in filename:\n",
    "        legit = 1\n",
    "    \n",
    "    with open(filename, \"r\") as file:\n",
    "        subject_words = file.readline()[9:-1].split()\n",
    "        file.readline()\n",
    "        message_words = file.readline()[:-1].split()\n",
    "        \n",
    "    return list(map(int, subject_words)), list(map(int, message_words)), legit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_from_lists(subject_words, message_words, n):\n",
    "    subject_ngrams = [tuple(subject_words[j:j+n]) for j in range(len(subject_words)-n+1)]\n",
    "    message_ngrams = [tuple(message_words[j:j+n]) for j in range(len(message_words)-n+1)]\n",
    "    \n",
    "    if not subject_ngrams:\n",
    "        return message_ngrams\n",
    "    \n",
    "    return subject_ngrams + message_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_from_ngrams(letter, ndict):\n",
    "    vec = np.zeros(len(ndict))\n",
    "    \n",
    "    for ngram in letter:\n",
    "        vec[ndict[ngram]] = 1\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prepared_data_from_dir(n, data_dirname='messages/'):\n",
    "    X = []\n",
    "    y = []\n",
    "    X_ngrams = []\n",
    "    all_ngrams = []\n",
    "\n",
    "    ngram_dict = {}\n",
    "\n",
    "    for dirname in os.listdir(data_dirname):\n",
    "        filenames = os.listdir(data_dirname + dirname)\n",
    "        full_filenames = [data_dirname + dirname + '/' + filename for filename in filenames]\n",
    "    \n",
    "        part_x = []\n",
    "        part_y = []\n",
    "        for name in full_filenames:\n",
    "            subject_words, message_words, legit = get_data_from_file(name)\n",
    "            cur_ngrams = get_ngrams_from_lists(subject_words, message_words, n)\n",
    "            part_x.append(cur_ngrams)\n",
    "            part_y.append(legit)\n",
    "            all_ngrams += cur_ngrams\n",
    "    \n",
    "        X_ngrams.append(part_x)\n",
    "        y.append(part_y)\n",
    "        \n",
    "    all_ngrams = list(set(all_ngrams))\n",
    "    \n",
    "    for i in range(len(all_ngrams)):\n",
    "        ngram_dict[all_ngrams[i]] = i\n",
    "    \n",
    "    for part in X_ngrams:\n",
    "        vpart = []\n",
    "        for letter in part:\n",
    "            vec = get_vector_from_ngrams(letter, ngram_dict)\n",
    "            vpart.append(vec)\n",
    "        X.append(vpart)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_a_priori_proba(labels):\n",
    "    legit = np.count_nonzero(labels)\n",
    "    return legit/len(labels), (len(labels)-legit)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laplas_proba(wcount, all_count, alpha):\n",
    "    return (wcount + alpha)/(all_count + alpha*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_cond_proba(X, y, alpha):\n",
    "    legit = []\n",
    "    spam = []\n",
    "    for i in range(len(X)):\n",
    "        if y[i] == 1:\n",
    "            legit.append(X[i])\n",
    "        else:\n",
    "            spam.append(X[i])\n",
    "    \n",
    "    nvec = len(X[0])\n",
    "    \n",
    "    vlegit = np.zeros(nvec)\n",
    "    vspam = np.zeros(nvec)\n",
    "    \n",
    "    for i in range(nvec):\n",
    "        flegit = list(filter(lambda x: x[i], legit))\n",
    "        fspam = list(filter(lambda x: not x[i], spam))\n",
    "        vlegit[i] = get_laplas_proba(len(flegit), len(legit), alpha)\n",
    "        vspam[i] = get_laplas_proba(len(fspam), len(spam), alpha)\n",
    "    \n",
    "    return vlegit, vspam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letter_proba(vletter, apri, vproba, lambda_):\n",
    "    lproba = [vproba[i] if vletter[i] else 1-vproba[i] for i in range(len(vletter))]\n",
    "    log_proba = list(map(log, lproba))\n",
    "    sum_cond_proba = sum(log_proba)\n",
    "    \n",
    "    return log(lambda_ * apri) + sum_cond_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letter_class(vletter, apri_legit, apri_spam, vlegit, vspam, lambda_legit, lambda_spam):\n",
    "    legit_proba = get_letter_proba(vletter, apri_legit, vlegit, lambda_legit)\n",
    "    spam_proba = get_letter_proba(vletter, apri_spam, vspam, lambda_spam)\n",
    "    \n",
    "    print(legit_proba, spam_proba)\n",
    "    \n",
    "    if (legit_proba > spam_proba):\n",
    "        return (1, legit_proba)\n",
    "\n",
    "    return (0, spam_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation and finding hyper-params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_and_legit_neg(y_actual, y_pred):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for (a, p) in zip(y_actual, y_pred):\n",
    "        if a == p:\n",
    "            if p == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if p == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "                \n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn) \n",
    "    \n",
    "    return accuracy, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(n):\n",
    "    X, y = get_prepared_data_from_dir(n)\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        cur_train_x = np.delete(X, i, 0)\n",
    "        cur_train_y = np.delete(y, i, 0)\n",
    "        cur_test_x = X[:][i]\n",
    "        cur_test_y = y[:][i]\n",
    "        train_x.append(np.concatenate(cur_train_x))\n",
    "        train_y.append(np.concatenate(cur_train_y))\n",
    "        test_x.append(cur_test_x)\n",
    "        test_y.append(cur_test_y)\n",
    "        \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [1]\n",
    "alphas = [0.0001]\n",
    "lambda_legit = 1\n",
    "lambda_spam = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clf with finding best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "min_ln = 100000\n",
    "best_params = []\n",
    "best_pred = []\n",
    "legit_neg = []\n",
    "accs = []\n",
    "\n",
    "with tqdm(total=len(ns)*len(alphas)*1, desc=\"clfprogress\", leave=False) as trp:\n",
    "    for n in ns:\n",
    "        for alpha in alphas:\n",
    "            accs = []\n",
    "            legit_neg = []\n",
    "            pred = []\n",
    "            train_x, train_y, test_x, test_y = get_train_test_data(n)\n",
    "            for i in range(1):\n",
    "                vlegit, vspam = get_all_words_cond_proba(train_x[i], train_y[i], alpha)\n",
    "                apri_legit, apri_spam = get_classes_a_priori_proba(train_y[i])\n",
    "                test_pred_y = list(map(lambda x: get_letter_class(x, apri_legit, apri_spam, vlegit, vspam, lambda_legit, lambda_spam), test_x[i]))\n",
    "                accuracy, ln = get_accuracy_and_legit_neg(test_y[i], list(zip(*test_pred_y))[0])\n",
    "                accs.append(accuracy)\n",
    "                legit_neg.append(ln)\n",
    "                pred.append(test_pred_y)\n",
    "\n",
    "                trp.update(1)\n",
    "            average_acc = sum(accs)/len(accs)\n",
    "            sum_ln = sum(legit_neg)\n",
    "            print(str(average_acc) + \" \" + str(sum_ln))\n",
    "            if sum_ln < min_ln or sum_ln == min_ln and average_acc > best_acc:\n",
    "                best_acc = average_acc\n",
    "                min_l = sum_ln\n",
    "                best_params = [n, alpha, lambda_legit]\n",
    "                best_pred = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_roc(pred):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"ROC Curve\")\n",
    "plt.plot(best_roc[0], best_roc[1])\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accs = []\n",
    "\n",
    "lambdas_legits = [1, 100, 1000, 10000]\n",
    "for ll in lambdas_legit:\n",
    "    accs = []\n",
    "    train_x, train_y, test_x, test_y, ndict = get_train_test_data_and_dict(best_params[0])\n",
    "    for i in range(10):\n",
    "        vlegit, vspam = get_all_words_cond_proba(train_x[i], train_y[i], ndict, best_params[1])\n",
    "        apri_legit, apri_spam = get_classes_a_priori_proba(train_y[i])\n",
    "        test_pred_y = list(map(lambda x: get_letter_class(x, apri_legit, apri_spam, vlegit, vspam, ll, lambda_spam), test_x[i]))\n",
    "        accuracy, ln, tpr, fpr = get_accuracy_and_legit_neg(test_y[i], test_pred_y)\n",
    "        accs.append(accuracy)\n",
    "    average_accs.append(sum(accs)/len(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambdas_legit, average_accs)\n",
    "plt.xlabel(\"lambda legit\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
